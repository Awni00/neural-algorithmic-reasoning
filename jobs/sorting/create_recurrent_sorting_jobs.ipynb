{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from dotenv import load_dotenv\n",
    "import copy\n",
    "import time\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "\n",
    "import os, sys; sys.path.insert(0, os.path.abspath('../..')) # add project root dir to path\n",
    "from experiments.sorting.recurrent_model import get_experiment_name\n",
    "from utils.utils import AttributeDict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def mkdir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global job parameters\n",
    "\n",
    "job_directory = f\"job_scripts\"\n",
    "out_dir = f'.out'\n",
    "time_str = '00-4:00:00'\n",
    "partition = 'gpu'\n",
    "ntasks = 1\n",
    "nodes = 1\n",
    "cpu_per_gpu = 8\n",
    "mem_per_cpu = 64\n",
    "n_gpus = 1\n",
    "\n",
    "cluster = 'misha'\n",
    "\n",
    "if cluster == 'grace':\n",
    "    gpus_constraints = '\"a100|rtx3090|v100|rtx2080ti\"' # for grace\n",
    "# gpus_constraints = \"a40\" #'\"h100|a100\"' # for misha\n",
    "\n",
    "netid = os.getenv('NETID')\n",
    "project_dir = f\"/home/{netid}/project/neural-algorithmic-reasoning/experiments/sorting\"\n",
    "\n",
    "mkdir(job_directory)\n",
    "mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model, train, and data config\n",
    "import yaml\n",
    "base_config_dir = f'{project_dir}/experiment_configs/base_config_recurrent'\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'model_config.yaml')) as f:\n",
    "    base_model_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'train_config.yaml')) as f:\n",
    "    base_train_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'data_config.yaml')) as f:\n",
    "    base_data_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "config_out_dir = f'{project_dir}/experiment_configs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 jobs\n"
     ]
    }
   ],
   "source": [
    "wandb_project = 'sorting-recurrent-resstream-exploration'\n",
    "\n",
    "n_layers = [2] # [2, 4]\n",
    "d_model = [256]\n",
    "dff_expansion = [2]\n",
    "\n",
    "# TODO: add variation over MLP activation functions, e.g. softmax linear unit, etc.\n",
    "# search over dff_expansion, or other MLP params\n",
    "\n",
    "random_train_length = [True] # , False\n",
    "pos_enc_type = ['rotary'] # , 't5', 'alibi', 'sinusoidal'\n",
    "input_recall = [True] # False\n",
    "attn_score_fn = ['adaptive-temperature-softmax'] # softmax, 'topk-softmax', 'hard', 'sigmoid']\n",
    "# discrete_intermediate_map = ['solu', 'softmax', 'topk-softmax', 'hard', 'gumbel-softmax'] # None,'sigmoid', 'relu'\n",
    "discrete_intermediate_map = [None, 'softmax', 'gumbel-softmax', 'hard', 'sigmoid'] # [None, 'softmax', 'hard', 'sigmoid', 'solu'] # 'sigmoid', 'relu'\n",
    "progressive_training = [True] # False\n",
    "incremental_training = [False] # True\n",
    "weight_tying_emb = [False] # [True, False]\n",
    "wt_intermediate = [False] # [True, False]\n",
    "\n",
    "predisc_norm = [False, True]\n",
    "postdisc_norm = [False, True]\n",
    "\n",
    "norm_config = [\n",
    "    dict(norm_method='none', norm_type='rmsnorm'),\n",
    "    dict(norm_method='pre-norm', norm_type='rmsnorm'),\n",
    "    dict(norm_method='post-norm', norm_type='rmsnorm'),\n",
    "    dict(norm_method='pre+post-norm', norm_type='rmsnorm'),\n",
    "    dict(norm_method='hypersphere-interpolation', norm_type='rmsnorm')\n",
    "    ]\n",
    "\n",
    "use_eos_bos = [True]\n",
    "\n",
    "for L, D, F, posenc, norm, ir, wt_emb, wt_interm, attn_fn, disc_map, prenorm, postnorm, progtr, inctr, randlen, eosbos in itertools.product(\n",
    "    n_layers, d_model, dff_expansion, pos_enc_type, norm_config, input_recall, weight_tying_emb, wt_intermediate, attn_score_fn, discrete_intermediate_map, predisc_norm, postdisc_norm, progressive_training, incremental_training, random_train_length, use_eos_bos):\n",
    "\n",
    "    # copy base configs\n",
    "    job_model_config = copy.deepcopy(base_model_config)\n",
    "    job_train_config = copy.deepcopy(base_train_config)\n",
    "    job_data_config = copy.deepcopy(base_data_config)\n",
    "\n",
    "    ### update model config\n",
    "\n",
    "    # attn_kwargs params\n",
    "    attn_kwargs = dict(attn_score_fn=attn_fn)\n",
    "    if attn_fn == 'topk-softmax':\n",
    "        attn_kwargs['attn_score_fn_params'] = dict(k=3, straight_through=True)\n",
    "    elif attn_fn == 'hard':\n",
    "        attn_kwargs['attn_score_fn_params'] = dict(straight_through=True)\n",
    "\n",
    "    # discrete_intermediate args\n",
    "    intermediate_discretization = dict(discrete_intermediate=(disc_map is not None))\n",
    "    if disc_map is not None:\n",
    "        intermediate_discretization['discretize_map'] = disc_map\n",
    "    match disc_map:\n",
    "        case 'gumbel-softmax':\n",
    "            intermediate_discretization['discretization_map_params'] = dict(tau=1, hard=False)\n",
    "        case 'hard':\n",
    "            intermediate_discretization['discretization_map_params'] = dict(straight_through=True)\n",
    "        case 'topk-softmax':\n",
    "            intermediate_discretization['discretization_map_params'] = dict(k=3, straight_through=True)\n",
    "\n",
    "    if disc_map is None and wt_interm:\n",
    "        continue\n",
    "\n",
    "    job_model_config.update(dict(\n",
    "        n_layers=L, d_model=D, dff=D*F,\n",
    "        pos_enc_type=posenc,\n",
    "        norm_config=norm,\n",
    "        input_recall=ir,\n",
    "        predisc_norm=prenorm,\n",
    "        postdisc_norm=postnorm,\n",
    "        weight_tie_embed_to_token=wt_emb,\n",
    "        weight_tie_discrete_interm=wt_interm,\n",
    "        attn_kwargs=attn_kwargs,\n",
    "        intermediate_discretization=intermediate_discretization\n",
    "        ))\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    ### Train config\n",
    "    job_train_config.update(dict(\n",
    "        progressive_training=progtr,\n",
    "        incremental_training=inctr,\n",
    "        wandb_config=(job_train_config['wandb_config'] | dict(wandb_project=wandb_project)),\n",
    "    ))\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    ### Data config\n",
    "    job_data_config.update(dict(\n",
    "        train_random_sequence_length=randlen,\n",
    "        include_bos_eos=eosbos\n",
    "    ))\n",
    "\n",
    "    if inctr and not progtr:\n",
    "        continue\n",
    "\n",
    "    job_config = dict(model_config=job_model_config, train_config=job_train_config, data_config=job_data_config)\n",
    "    job_config = AttributeDict(job_config)\n",
    "    jobs_overwrite_params.append(job_config)\n",
    "\n",
    "print(f\"Generated {len(jobs_overwrite_params)} jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_config(config_upate, out_dir, uid=None):\n",
    "    global base_model_config, base_train_config, base_data_config\n",
    "    model_config, train_config, data_config = tuple(copy.deepcopy(c) for c in (base_model_config, base_train_config, base_data_config))\n",
    "\n",
    "    model_config.update(config_upate.get('model_config', {}))\n",
    "    train_config.update(config_upate.get('train_config', {}))\n",
    "    data_config.update(config_upate.get('data_config', {}))\n",
    "\n",
    "    experiment_name, _ = get_experiment_name(model_config, data_config, train_config)\n",
    "    experiment_name = experiment_name.replace(' ', '')\n",
    "    if uid is not None:\n",
    "        experiment_name = f\"UID{uid}-{experiment_name}\"\n",
    "\n",
    "    mkdir(os.path.join(out_dir, experiment_name))\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/model_config.yaml'), 'w') as f:\n",
    "        yaml.dump(model_config.todict(), f)\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/train_config.yaml'), 'w') as f:\n",
    "        yaml.dump(train_config.todict(), f)\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/data_config.yaml'), 'w') as f:\n",
    "        yaml.dump(data_config.todict(), f)\n",
    "\n",
    "    return model_config, train_config, data_config, experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_script(experiment_name):\n",
    "    filename = f'{job_directory}/{experiment_name}.job'\n",
    "    with open(filename, 'w') as fh:\n",
    "        fh.writelines(f\"#!/bin/bash\\n\")\n",
    "        fh.writelines(f\"#SBATCH --partition={partition}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --job-name={experiment_name}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --output={out_dir}/%j-{experiment_name}.out\\n\")\n",
    "        fh.writelines(f\"#SBATCH --ntasks={ntasks} --nodes={nodes}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --cpus-per-gpu={cpu_per_gpu}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mem-per-cpu={mem_per_cpu}G\\n\")\n",
    "        fh.writelines(f\"#SBATCH --time={time_str}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mail-type=ALL\\n\")\n",
    "        fh.writelines(f\"#SBATCH --gres=gpu:{n_gpus}\\n\")\n",
    "        # fh.writelines(f\"#SBATCH --constraint={gpus_constraints}\\n\")\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines('module load StdEnv\\n')\n",
    "        fh.writelines('export SLURM_EXPORT_ENV=ALL\\n')\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        if cluster == 'grace':\n",
    "            fh.writelines(f\"module restore python_env\\n\") # load modules i need\n",
    "        elif cluster == 'misha':\n",
    "            fh.writelines(f\"module load miniconda\\n\") # load modules i need\n",
    "        else:\n",
    "            raise ValueError(f\"Cluster {cluster} not supported\")\n",
    "\n",
    "        # fh.writelines(f\"conda init\\n\")\n",
    "        fh.writelines(f\"conda activate neural_prog\\n\") # activate conda environment\n",
    "        fh.writelines(f\"conda info --envs\\n\") # activate conda environment\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines(f\"nvidia-smi -L\\n\") # print gpu information\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        fh.writelines(f\"cd {project_dir}\\n\") # navigate to project directory\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        # run python script\n",
    "        fh.writelines(f\"srun python train_recurrent.py --config_dir experiment_configs/{experiment_name}\\n\") # run python script\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name: UID0-HASH-1166851971076522499\n",
      "Experiment Name: UID1-HASH--791047997625647921\n",
      "Experiment Name: UID2-HASH--8947547962214129698\n",
      "Experiment Name: UID3-HASH-1939744671199455593\n",
      "Experiment Name: UID4-HASH--1096642177693627903\n",
      "Experiment Name: UID5-HASH-6848552955685120694\n",
      "Experiment Name: UID6-HASH--7741991760838337767\n",
      "Experiment Name: UID7-HASH--1511231792858688456\n",
      "Experiment Name: UID8-HASH--8824826762611496564\n",
      "Experiment Name: UID9-HASH--5617723092104166830\n",
      "Experiment Name: UID10-HASH--7800834381702155406\n",
      "Experiment Name: UID11-HASH-1887772761741376285\n",
      "Experiment Name: UID12-HASH-3346903417947276123\n",
      "Experiment Name: UID13-HASH-29069865286714795\n",
      "Experiment Name: UID14-HASH--7426031391859039041\n",
      "Experiment Name: UID15-HASH--5322354303423183792\n",
      "Experiment Name: UID16-HASH--1348160817266416299\n",
      "Experiment Name: UID17-HASH-6601401082330057736\n",
      "Experiment Name: UID18-HASH-2363302450390050102\n",
      "Experiment Name: UID19-HASH-8711659510561927199\n",
      "Experiment Name: UID20-HASH-1166851971076522499\n",
      "Experiment Name: UID21-HASH--791047997625647921\n",
      "Experiment Name: UID22-HASH--8947547962214129698\n",
      "Experiment Name: UID23-HASH-1939744671199455593\n",
      "Experiment Name: UID24-HASH--1096642177693627903\n",
      "Experiment Name: UID25-HASH-6848552955685120694\n",
      "Experiment Name: UID26-HASH--7741991760838337767\n",
      "Experiment Name: UID27-HASH--1511231792858688456\n",
      "Experiment Name: UID28-HASH--8824826762611496564\n",
      "Experiment Name: UID29-HASH--5617723092104166830\n",
      "Experiment Name: UID30-HASH--7800834381702155406\n",
      "Experiment Name: UID31-HASH-1887772761741376285\n",
      "Experiment Name: UID32-HASH-3346903417947276123\n",
      "Experiment Name: UID33-HASH-29069865286714795\n",
      "Experiment Name: UID34-HASH--7426031391859039041\n",
      "Experiment Name: UID35-HASH--5322354303423183792\n",
      "Experiment Name: UID36-HASH--1348160817266416299\n",
      "Experiment Name: UID37-HASH-6601401082330057736\n",
      "Experiment Name: UID38-HASH-2363302450390050102\n",
      "Experiment Name: UID39-HASH-8711659510561927199\n",
      "Experiment Name: UID40-HASH-1166851971076522499\n",
      "Experiment Name: UID41-HASH--791047997625647921\n",
      "Experiment Name: UID42-HASH--8947547962214129698\n",
      "Experiment Name: UID43-HASH-1939744671199455593\n",
      "Experiment Name: UID44-HASH--1096642177693627903\n",
      "Experiment Name: UID45-HASH-6848552955685120694\n",
      "Experiment Name: UID46-HASH--7741991760838337767\n",
      "Experiment Name: UID47-HASH--1511231792858688456\n",
      "Experiment Name: UID48-HASH--8824826762611496564\n",
      "Experiment Name: UID49-HASH--5617723092104166830\n",
      "Experiment Name: UID50-HASH--7800834381702155406\n",
      "Experiment Name: UID51-HASH-1887772761741376285\n",
      "Experiment Name: UID52-HASH-3346903417947276123\n",
      "Experiment Name: UID53-HASH-29069865286714795\n",
      "Experiment Name: UID54-HASH--7426031391859039041\n",
      "Experiment Name: UID55-HASH--5322354303423183792\n",
      "Experiment Name: UID56-HASH--1348160817266416299\n",
      "Experiment Name: UID57-HASH-6601401082330057736\n",
      "Experiment Name: UID58-HASH-2363302450390050102\n",
      "Experiment Name: UID59-HASH-8711659510561927199\n",
      "Experiment Name: UID60-HASH-1166851971076522499\n",
      "Experiment Name: UID61-HASH--791047997625647921\n",
      "Experiment Name: UID62-HASH--8947547962214129698\n",
      "Experiment Name: UID63-HASH-1939744671199455593\n",
      "Experiment Name: UID64-HASH--1096642177693627903\n",
      "Experiment Name: UID65-HASH-6848552955685120694\n",
      "Experiment Name: UID66-HASH--7741991760838337767\n",
      "Experiment Name: UID67-HASH--1511231792858688456\n",
      "Experiment Name: UID68-HASH--8824826762611496564\n",
      "Experiment Name: UID69-HASH--5617723092104166830\n",
      "Experiment Name: UID70-HASH--7800834381702155406\n",
      "Experiment Name: UID71-HASH-1887772761741376285\n",
      "Experiment Name: UID72-HASH-3346903417947276123\n",
      "Experiment Name: UID73-HASH-29069865286714795\n",
      "Experiment Name: UID74-HASH--7426031391859039041\n",
      "Experiment Name: UID75-HASH--5322354303423183792\n",
      "Experiment Name: UID76-HASH--1348160817266416299\n",
      "Experiment Name: UID77-HASH-6601401082330057736\n",
      "Experiment Name: UID78-HASH-2363302450390050102\n",
      "Experiment Name: UID79-HASH-8711659510561927199\n",
      "Experiment Name: UID80-HASH-1166851971076522499\n",
      "Experiment Name: UID81-HASH--791047997625647921\n",
      "Experiment Name: UID82-HASH--8947547962214129698\n",
      "Experiment Name: UID83-HASH-1939744671199455593\n",
      "Experiment Name: UID84-HASH--1096642177693627903\n",
      "Experiment Name: UID85-HASH-6848552955685120694\n",
      "Experiment Name: UID86-HASH--7741991760838337767\n",
      "Experiment Name: UID87-HASH--1511231792858688456\n",
      "Experiment Name: UID88-HASH--8824826762611496564\n",
      "Experiment Name: UID89-HASH--5617723092104166830\n",
      "Experiment Name: UID90-HASH--7800834381702155406\n",
      "Experiment Name: UID91-HASH-1887772761741376285\n",
      "Experiment Name: UID92-HASH-3346903417947276123\n",
      "Experiment Name: UID93-HASH-29069865286714795\n",
      "Experiment Name: UID94-HASH--7426031391859039041\n",
      "Experiment Name: UID95-HASH--5322354303423183792\n",
      "Experiment Name: UID96-HASH--1348160817266416299\n",
      "Experiment Name: UID97-HASH-6601401082330057736\n",
      "Experiment Name: UID98-HASH-2363302450390050102\n",
      "Experiment Name: UID99-HASH-8711659510561927199\n"
     ]
    }
   ],
   "source": [
    "job_script_files = []\n",
    "\n",
    "for uid, job_params in enumerate(jobs_overwrite_params):\n",
    "    base_model_config, base_train_config, base_data_config, experiment_name = create_job_config(job_params, config_out_dir, uid=uid)\n",
    "\n",
    "    print(f\"Experiment Name: {experiment_name}\")\n",
    "\n",
    "    job_script = create_job_script(experiment_name)\n",
    "    job_script_files.append(job_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not submitting jobs\n"
     ]
    }
   ],
   "source": [
    "wait_time = 0.5 # number of seconds to wait between job submissions\n",
    "n_trials = 1\n",
    "\n",
    "confirm = input(\"Do you want to submit the jobs? (y/n): \")\n",
    "\n",
    "responses = []\n",
    "\n",
    "if confirm == 'y':\n",
    "    for ir in range(n_trials):\n",
    "        print('Trial:', ir)\n",
    "        for job_script in job_script_files:\n",
    "            response = subprocess.run(['sbatch', job_script], capture_output=True)\n",
    "            print(f\"response: {response.stdout.decode('utf-8').strip()}, return_code={response.returncode}, job_script={job_script}\")\n",
    "            responses.append(response)\n",
    "            time.sleep(wait_time)\n",
    "        print()\n",
    "else:\n",
    "    print(\"Not submitting jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any jobs failed to submit\n",
    "for response in responses:\n",
    "    if not response.stdout.decode('utf-8').startswith('Submitted batch job') or response.returncode != 0:\n",
    "        print(f\"Failed to submit job: {response.stdout.decode('utf-8')}\")\n",
    "        print(f\"stderr: {response.stderr.decode('utf-8')}\")\n",
    "        print(f\"Full response: {response}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
