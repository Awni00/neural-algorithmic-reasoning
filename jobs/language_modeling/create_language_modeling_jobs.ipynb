{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from dotenv import load_dotenv\n",
    "import copy\n",
    "import time\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "\n",
    "import os, sys; sys.path.insert(0, os.path.abspath('../..')) # add project root dir to path\n",
    "from experiments.language_modeling.model import get_experiment_name\n",
    "from utils.utils import AttributeDict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def mkdir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global job parameters\n",
    "\n",
    "job_directory = f\"job_scripts\"\n",
    "out_dir = f'.out'\n",
    "\n",
    "time_str = '00-24:00:00'\n",
    "max_time = '00:23:55:00' # 5 minutes less than the time_str; this is the format PL uses\n",
    "\n",
    "partition = 'gpu'\n",
    "ntasks = 1\n",
    "nodes = 1\n",
    "cpu_per_gpu = 8\n",
    "mem_per_cpu = 8\n",
    "n_gpus = 1\n",
    "\n",
    "cluster = 'misha'\n",
    "\n",
    "if cluster == 'grace':\n",
    "    gpus_constraints = '\"a100|rtx3090|v100|rtx2080ti\"' # for grace\n",
    "# gpus_constraints = \"a40\" #'\"h100|a100\"' # for misha\n",
    "\n",
    "netid = os.getenv('NETID')\n",
    "project_dir = f\"/home/{netid}/project/neural-algorithmic-reasoning/experiments/language_modeling\"\n",
    "\n",
    "mkdir(job_directory)\n",
    "mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model, train, and data config\n",
    "import yaml\n",
    "base_config_dir = f'{project_dir}/experiment_configs/base_config'\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'model_config.yaml')) as f:\n",
    "    base_model_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'train_config.yaml')) as f:\n",
    "    base_train_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "with open(os.path.join(base_config_dir, 'data_config.yaml')) as f:\n",
    "    base_data_config = AttributeDict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "\n",
    "config_out_dir = f'{project_dir}/experiment_configs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = dict(\n",
    "    n_layers=[6], d_model=[256], dff_expansion=[2],\n",
    "    pos_enc_type = ['rotary', 'none'], # , 't5', 'alibi', 'sinusoidal'\n",
    "    attn_score_fn = ['softmax'], # softmax, 'adaptive-temperature-softmax', 'topk-softmax', 'hard', 'sigmoid']\n",
    "    norm_config = [\n",
    "        dict(norm_method='none', norm_type='rmsnorm'),\n",
    "        dict(norm_method='pre-norm', norm_type='rmsnorm'),\n",
    "        dict(norm_method='post-norm', norm_type='rmsnorm'),\n",
    "        dict(norm_method='pre+post-norm', norm_type='rmsnorm'),\n",
    "        dict(norm_method='hypersphere-interpolation', lerp_weight_constraint='none'),\n",
    "        dict(norm_method='hypersphere-spherical-interpolation', single_weight=True),\n",
    "        dict(norm_method='adaptive-hypersphere-interpolation', single_weight=True),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "wandb_project = 'language-modeling'\n",
    "\n",
    "sequence_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of jobs 14\n"
     ]
    }
   ],
   "source": [
    "jobs_overwrite_params = []\n",
    "\n",
    "for model_config_update in itertools.product(*[[(k, v) for v in vs] for k, vs in model_configs.items()]):\n",
    "    # copy base configs\n",
    "    job_model_config = copy.deepcopy(base_model_config)\n",
    "    job_train_config = copy.deepcopy(base_train_config)\n",
    "    job_data_config = copy.deepcopy(base_data_config)\n",
    "\n",
    "    # update model config\n",
    "    for k, v in model_config_update:\n",
    "        job_model_config[k] = v\n",
    "\n",
    "    # update train config\n",
    "    job_train_config['wandb_config'] = job_train_config['wandb_config'] | dict(wandb_project=wandb_project)\n",
    "\n",
    "    job_train_config['max_time'] = max_time\n",
    "\n",
    "    # update data config\n",
    "    job_data_config['sequence_length'] = sequence_length\n",
    "\n",
    "    job_config = dict(model_config=job_model_config, train_config=job_train_config, data_config=job_data_config)\n",
    "    job_config = AttributeDict(job_config)\n",
    "    jobs_overwrite_params.append(job_config)\n",
    "\n",
    "print('number of jobs', len(jobs_overwrite_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_config(config_upate, out_dir, uid=None):\n",
    "    global base_model_config, base_train_config, base_data_config\n",
    "    model_config, train_config, data_config = tuple(copy.deepcopy(c) for c in (base_model_config, base_train_config, base_data_config))\n",
    "\n",
    "    model_config.update(config_upate.get('model_config', {}))\n",
    "    train_config.update(config_upate.get('train_config', {}))\n",
    "    data_config.update(config_upate.get('data_config', {}))\n",
    "\n",
    "    experiment_name, _ = get_experiment_name(model_config, data_config, train_config)\n",
    "    experiment_name = experiment_name.replace(' ', '')\n",
    "    if uid is not None:\n",
    "        experiment_name = f\"UID{uid}-{experiment_name}\"\n",
    "\n",
    "    mkdir(os.path.join(out_dir, experiment_name))\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/model_config.yaml'), 'w') as f:\n",
    "        yaml.dump(model_config.todict(), f)\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/train_config.yaml'), 'w') as f:\n",
    "        yaml.dump(train_config.todict(), f)\n",
    "\n",
    "    with open(os.path.join(out_dir, f'{experiment_name}/data_config.yaml'), 'w') as f:\n",
    "        yaml.dump(data_config.todict(), f)\n",
    "\n",
    "    return model_config, train_config, data_config, experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_script(experiment_name):\n",
    "    filename = f'{job_directory}/{experiment_name}.job'\n",
    "    with open(filename, 'w') as fh:\n",
    "        fh.writelines(f\"#!/bin/bash\\n\")\n",
    "        fh.writelines(f\"#SBATCH --partition={partition}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --job-name={experiment_name}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --output={out_dir}/%j-{experiment_name}.out\\n\")\n",
    "        fh.writelines(f\"#SBATCH --ntasks={ntasks} --nodes={nodes}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --cpus-per-gpu={cpu_per_gpu}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mem-per-cpu={mem_per_cpu}G\\n\")\n",
    "        fh.writelines(f\"#SBATCH --time={time_str}\\n\")\n",
    "        fh.writelines(f\"#SBATCH --mail-type=ALL\\n\")\n",
    "        fh.writelines(f\"#SBATCH --gpus={n_gpus}\\n\")\n",
    "        # fh.writelines(f\"#SBATCH --constraint={gpus_constraints}\\n\")\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines('module load StdEnv\\n')\n",
    "        fh.writelines('export SLURM_EXPORT_ENV=ALL\\n')\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        if cluster == 'grace':\n",
    "            fh.writelines(f\"module restore python_env\\n\") # load modules i need\n",
    "        elif cluster == 'misha':\n",
    "            fh.writelines(f\"module load miniconda\\n\") # load modules i need\n",
    "        else:\n",
    "            raise ValueError(f\"Cluster {cluster} not supported\")\n",
    "\n",
    "        # fh.writelines(f\"conda init\\n\")\n",
    "        fh.writelines(f\"conda activate neural_prog\\n\") # activate conda environment\n",
    "        fh.writelines(f\"conda info --envs\\n\") # activate conda environment\n",
    "\n",
    "        fh.writelines('\\n')\n",
    "        fh.writelines(f\"nvidia-smi -L\\n\") # print gpu information\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        fh.writelines(f\"cd {project_dir}\\n\") # navigate to project directory\n",
    "        fh.writelines('\\n')\n",
    "\n",
    "        # run python script\n",
    "        fh.writelines(f\"srun python train.py --config_dir experiment_configs/{experiment_name}\\n\") # run python script\n",
    "\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name: UID0-L6H8D256-rotary-none-WTTrue\n",
      "Experiment Name: UID1-L6H8D256-rotary-pre-norm-WTTrue\n",
      "Experiment Name: UID2-L6H8D256-rotary-post-norm-WTTrue\n",
      "Experiment Name: UID3-L6H8D256-rotary-pre+post-norm-WTTrue\n",
      "Experiment Name: UID4-L6H8D256-rotary-hypersphere-interpolation-WTTrue\n",
      "Experiment Name: UID5-L6H8D256-rotary-hypersphere-spherical-interpolation-WTTrue\n",
      "Experiment Name: UID6-L6H8D256-rotary-adaptive-hypersphere-interpolation-WTTrue\n",
      "Experiment Name: UID7-L6H8D256-none-none-WTTrue\n",
      "Experiment Name: UID8-L6H8D256-none-pre-norm-WTTrue\n",
      "Experiment Name: UID9-L6H8D256-none-post-norm-WTTrue\n",
      "Experiment Name: UID10-L6H8D256-none-pre+post-norm-WTTrue\n",
      "Experiment Name: UID11-L6H8D256-none-hypersphere-interpolation-WTTrue\n",
      "Experiment Name: UID12-L6H8D256-none-hypersphere-spherical-interpolation-WTTrue\n",
      "Experiment Name: UID13-L6H8D256-none-adaptive-hypersphere-interpolation-WTTrue\n"
     ]
    }
   ],
   "source": [
    "job_script_files = []\n",
    "\n",
    "for uid, job_params in enumerate(jobs_overwrite_params):\n",
    "    base_model_config, base_train_config, base_data_config, experiment_name = create_job_config(job_params, config_out_dir, uid=uid)\n",
    "\n",
    "    print(f\"Experiment Name: {experiment_name}\")\n",
    "\n",
    "    job_script = create_job_script(experiment_name)\n",
    "    job_script_files.append(job_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0\n",
      "response: Submitted batch job 137401, return_code=0, job_script=job_scripts/UID0-L6H8D256-rotary-none-WTTrue.job\n",
      "response: Submitted batch job 137402, return_code=0, job_script=job_scripts/UID1-L6H8D256-rotary-pre-norm-WTTrue.job\n",
      "response: Submitted batch job 137403, return_code=0, job_script=job_scripts/UID2-L6H8D256-rotary-post-norm-WTTrue.job\n",
      "response: Submitted batch job 137404, return_code=0, job_script=job_scripts/UID3-L6H8D256-rotary-pre+post-norm-WTTrue.job\n",
      "response: Submitted batch job 137405, return_code=0, job_script=job_scripts/UID4-L6H8D256-rotary-hypersphere-interpolation-WTTrue.job\n",
      "response: Submitted batch job 137406, return_code=0, job_script=job_scripts/UID5-L6H8D256-rotary-hypersphere-spherical-interpolation-WTTrue.job\n",
      "response: Submitted batch job 137407, return_code=0, job_script=job_scripts/UID6-L6H8D256-rotary-adaptive-hypersphere-interpolation-WTTrue.job\n",
      "response: Submitted batch job 137408, return_code=0, job_script=job_scripts/UID7-L6H8D256-none-none-WTTrue.job\n",
      "response: Submitted batch job 137409, return_code=0, job_script=job_scripts/UID8-L6H8D256-none-pre-norm-WTTrue.job\n",
      "response: Submitted batch job 137410, return_code=0, job_script=job_scripts/UID9-L6H8D256-none-post-norm-WTTrue.job\n",
      "response: Submitted batch job 137411, return_code=0, job_script=job_scripts/UID10-L6H8D256-none-pre+post-norm-WTTrue.job\n",
      "response: Submitted batch job 137412, return_code=0, job_script=job_scripts/UID11-L6H8D256-none-hypersphere-interpolation-WTTrue.job\n",
      "response: Submitted batch job 137413, return_code=0, job_script=job_scripts/UID12-L6H8D256-none-hypersphere-spherical-interpolation-WTTrue.job\n",
      "response: Submitted batch job 137414, return_code=0, job_script=job_scripts/UID13-L6H8D256-none-adaptive-hypersphere-interpolation-WTTrue.job\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wait_time = 0.5 # number of seconds to wait between job submissions\n",
    "n_trials = 1\n",
    "\n",
    "confirm = input(\"Do you want to submit the jobs? (y/n): \")\n",
    "\n",
    "responses = []\n",
    "\n",
    "if confirm == 'y':\n",
    "    for ir in range(n_trials):\n",
    "        print('Trial:', ir)\n",
    "        for job_script in job_script_files:\n",
    "            response = subprocess.run(['sbatch', job_script], capture_output=True)\n",
    "            print(f\"response: {response.stdout.decode('utf-8').strip()}, return_code={response.returncode}, job_script={job_script}\")\n",
    "            responses.append(response)\n",
    "            time.sleep(wait_time)\n",
    "        print()\n",
    "else:\n",
    "    print(\"Not submitting jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any jobs failed to submit\n",
    "for response in responses:\n",
    "    if not response.stdout.decode('utf-8').startswith('Submitted batch job') or response.returncode != 0:\n",
    "        print(f\"Failed to submit job: {response.stdout.decode('utf-8')}\")\n",
    "        print(f\"stderr: {response.stderr.decode('utf-8')}\")\n",
    "        print(f\"Full response: {response}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
